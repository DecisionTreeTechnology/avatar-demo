<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>TalkingHead + HeadTTS Demo</title>
  <style>body {margin:0; background:#111;}</style>
</head>
<body>
  <div id="stage" style="width:100vw; height:100vh; position:relative;"></div>
  <div style="position:absolute; top:10px; left:10px; right:10px; z-index:10; background:rgba(0,0,0,0.55); padding:10px 12px; border-radius:10px; font:14px system-ui; color:#eee; max-width:760px;">
    <div style="display:flex; gap:8px; align-items:center;">
      <input id="text" value="Ask me anything about Azure OpenAI." placeholder="Type your question..." style="flex:1; background:#1d1f24; color:#eee; border:1px solid #333; padding:8px 10px; border-radius:6px;"/>
      <button id="speak" style="padding:8px 16px; background:#2d6ae0; border:none; color:#fff; border-radius:6px; cursor:pointer;">Ask</button>
      <span id="status" style="margin-left:4px; opacity:.8; min-width:90px;">Loading...</span>
    </div>
    <div id="answer" style="margin-top:8px; font:13px/1.4 system-ui; white-space:pre-wrap; color:#d7e4ff; max-height:140px; overflow:auto; display:none;"></div>
    <div style="margin-top:6px; font-size:11px; opacity:.6;">LLM + TTS demo. Configure window.AZURE_OPENAI_ENDPOINT / KEY / DEPLOYMENT (or Vite env) to enable server answers. Otherwise it will just TTS your input.</div>
  </div>

  <script type="module">
  import { TalkingHead } from '@met4citizen/talkinghead';
  import * as THREE from 'three';
  import * as SpeechSDK from 'microsoft-cognitiveservices-speech-sdk';
  // Using direct fetch for Azure OpenAI (SDK import removed due to browser export issue).

    const container = document.getElementById('stage');
    const status = document.getElementById('status');

    // TalkingHead requires a ttsEndpoint even if we only use speakAudio; provide dummy.
    let head;
    try {
      head = new TalkingHead(container, {
        ttsEndpoint: '/gtts/', // placeholder, we won't call speakText()
        // Enable English lipsync module so speakAudio can auto-generate visemes from words.
        lipsyncModules: ['en'],
        avatarMood: 'neutral',
        cameraView: 'head'
      });
      console.log('[diag] TalkingHead constructed');
      setTimeout(()=>{
        console.log('[diag] container canvas elements after construct', container.querySelectorAll('canvas'));
      }, 250);
    } catch(err) {
      console.error('TalkingHead init failed:', err);
      status.textContent = 'Init error (see console)';
      throw err;
    }

    // Load avatar (showAvatar expects an object with url)
    // Added morphTargets params to ensure viseme & face shapes are present.
    async function loadAvatarWithFallback() {
      const sources = [
        {label:'local avatar.glb', url:'./avatar.glb'},
        {label:'remote RPM fallback', url:'https://models.readyplayer.me/64a1c6a9a7a1d200140b4abc.glb?morphTargets=ARKit,Oculus+Visemes,mouthOpen,mouthSmile,eyesClosed,eyesLookUp,eyesLookDown&textureSizeLimit=1024&textureFormat=png'},
      ];
      let lastErr;
      for(const s of sources){
        try {
          status.textContent = `Loading avatar (${s.label})...`;
          await head.showAvatar({ url: s.url, lipsyncLang: 'en', avatarMood: 'neutral' });
          status.textContent = `Avatar loaded (${s.label})`;
          console.log('[diag] Loaded', s.label, 'url=', s.url);
          return;
        } catch(e){
          console.warn('[diag] Failed to load', s.label, e);
          lastErr = e;
        }
      }
      status.textContent = 'Avatar load error (all sources failed)';
      throw lastErr || new Error('No avatar sources loaded');
    }
  await loadAvatarWithFallback();
  console.log('[diag] Avatar load complete, armature=', !!head.armature);
  if(!head.armature){
    console.warn('[diag] Armature missing. Attempting manual skeleton root detection for basic visualization.');
    try {
      // Find first SkinnedMesh in scene and expose for debugging.
      const skinned = [];
      head.scene && head.scene.traverse(obj=>{ if(obj.isSkinnedMesh) skinned.push(obj); });
      console.log('[diag] Skinned meshes found:', skinned.length, skinned.map(m=>m.name));
      if(skinned.length){
        window._debug.firstSkinned = skinned[0];
        // Show its geometry bounding box for confirmation.
        const boxHelper = new THREE.Box3Helper(new THREE.Box3().setFromObject(skinned[0]), 0xff8800);
        head.scene.add(boxHelper);
        console.log('[diag] Added Box3Helper around first skinned mesh');
      } else {
        console.warn('[diag] No SkinnedMesh found; avatar file may be simple geometry without skeleton (e.g., Box.glb).');
      }
    } catch(e){ console.warn('[diag] skeleton detection failed', e); }
  }
  // Inspect scene and camera
  console.log('[diag] Scene children count', head.scene?.children?.length, head.scene?.children?.map(c=>c.type));
  console.log('[diag] Camera info', head.camera?.position, head.camera?.near, head.camera?.far);
  // Attempt to frame avatar manually if possible
  try {
    const box = new THREE.Box3();
    head.scene && box.setFromObject(head.scene);
    if(box.isEmpty() === false) {
      const size = new THREE.Vector3();
      const center = new THREE.Vector3();
      box.getSize(size); box.getCenter(center);
      if(head.camera) {
        head.camera.position.set(center.x, center.y + size.y*0.1, center.z + Math.max(size.x,size.y)*1.2);
        head.camera.lookAt(center);
        console.log('[diag] Reframed camera to box center', center, 'size', size, 'new cam pos', head.camera.position);
      }
    } else {
      console.log('[diag] Bounding box empty; will re-evaluate in 1s');
      setTimeout(()=>{
        try {
          const box2 = new THREE.Box3();
          head.scene && box2.setFromObject(head.scene);
          if(!box2.isEmpty() && head.camera) {
            const size2 = new THREE.Vector3();
            const center2 = new THREE.Vector3();
            box2.getSize(size2); box2.getCenter(center2);
            head.camera.position.set(center2.x, center2.y + size2.y*0.1, center2.z + Math.max(size2.x,size2.y)*1.2);
            head.camera.lookAt(center2);
            console.log('[diag] (retry) Reframed camera', center2, size2, head.camera.position);
          }
        } catch(e){ console.warn('[diag] retry frame error', e); }
      }, 1000);
    }
  } catch(e) {
    console.warn('[diag] framing error', e);
  }

    // Azure Speech SDK setup (values sourced from Vite env or window fallbacks)
    const AZURE_SPEECH_KEY = import.meta.env.VITE_AZURE_SPEECH_KEY || window.AZURE_SPEECH_KEY || '';
    const AZURE_SPEECH_REGION = import.meta.env.VITE_AZURE_SPEECH_REGION || window.AZURE_SPEECH_REGION || '';
    const AZURE_SPEECH_VOICE = import.meta.env.VITE_AZURE_SPEECH_VOICE || window.AZURE_SPEECH_VOICE || 'en-US-JennyNeural';
    if (!AZURE_SPEECH_KEY || !AZURE_SPEECH_REGION) {
      console.warn('[azure] Speech key/region not set. Provide VITE_AZURE_SPEECH_KEY & VITE_AZURE_SPEECH_REGION in .env (not committed).');
      status.textContent = 'Missing Azure Speech env vars';
    }
  const speechConfig = SpeechSDK.SpeechConfig.fromSubscription(AZURE_SPEECH_KEY, AZURE_SPEECH_REGION);
  speechConfig.speechSynthesisVoiceName = AZURE_SPEECH_VOICE;
  // Use RIFF 22kHz to simplify decode and match default lip-sync expectations.
  speechConfig.speechSynthesisOutputFormat = SpeechSDK.SpeechSynthesisOutputFormat.Riff22050Hz16BitMonoPcm;
    // Allow toggling MP3 fallback via window.USE_MP3_FALLBACK = true
  const USE_MP3_FALLBACK = (import.meta.env.VITE_USE_MP3_FALLBACK === 'true') || window.USE_MP3_FALLBACK;
  if (USE_MP3_FALLBACK) {
      console.log('[azure] Using MP3 fallback output format');
      speechConfig.speechSynthesisOutputFormat = SpeechSDK.SpeechSynthesisOutputFormat.Audio24Khz48KBitRateMonoMp3;
    }
    speechConfig.setProperty(SpeechSDK.PropertyId.SpeechServiceResponse_RequestWordBoundary, 'true');
    speechConfig.setProperty(SpeechSDK.PropertyId.SpeechServiceResponse_RequestViseme, 'true');
  // Pass no audioConfig => audioData returned in result; we handle playback ourselves.
  const synthesizer = new SpeechSDK.SpeechSynthesizer(speechConfig, undefined);
  // Verbose event logging for diagnosis
  synthesizer.synthesisStarted = () => console.log('[azure] synthesisStarted');
  synthesizer.synthesisCompleted = () => console.log('[azure] synthesisCompleted');
  synthesizer.synthesisCanceled = (s,e) => console.warn('[azure] synthesisCanceled', e);
  synthesizer.synthesisVisemeEvent = (s,e) => {/* already handled by visemeReceived */};
    window._debug = { head, synthesizer, SpeechSDK };
    status.textContent = 'Ready (Azure)';
    console.log('Initialization complete (Azure). Debug handle at window._debug');

    // LLM (Azure AI Foundry deployment running Mistral model)
    const AZURE_LLM_ENDPOINT = import.meta.env.VITE_AZURE_OPENAI_ENDPOINT || window.AZURE_OPENAI_ENDPOINT || '';
    const AZURE_LLM_KEY = import.meta.env.VITE_AZURE_OPENAI_KEY || window.AZURE_OPENAI_KEY || '';
    const AZURE_LLM_DEPLOYMENT = import.meta.env.VITE_AZURE_OPENAI_DEPLOYMENT || window.AZURE_OPENAI_DEPLOYMENT || '';
    const AZURE_LLM_API_VERSION = import.meta.env.VITE_AZURE_OPENAI_API_VERSION || '2024-05-01-preview';
    const systemPrompt = 'You are a concise, supportive fertility education assistant. Keep answers ≤150 words, plain language, and end with: "This information is for educational purposes only and is not a substitute for medical advice. Please consult your doctor or fertility specialist for personalized guidance."';
    async function fetchLLMAnswer(question){
      if(!AZURE_LLM_ENDPOINT || !AZURE_LLM_KEY || !AZURE_LLM_DEPLOYMENT) return null;
      const url = AZURE_LLM_ENDPOINT.replace(/\/$/, '') + `/openai/deployments/${AZURE_LLM_DEPLOYMENT}/chat/completions?api-version=${AZURE_LLM_API_VERSION}`;
      const base = {
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: question }
        ]
      };
      async function attempt(payload){
        const res = await fetch(url, { method:'POST', headers:{ 'Content-Type':'application/json', 'api-key': AZURE_LLM_KEY }, body: JSON.stringify(payload) });
        return { res, json: res.ok ? await res.json() : null };
      }
      try {
        // Try modern (some Azure models now expect max_tokens) first.
        let { res, json } = await attempt({ ...base, max_tokens: 600 });
        if(res.status === 400){
          const text = await res.text();
            if(/max_tokens/i.test(text) && /not supported|Use 'max_completion_tokens'/i.test(text)){
              console.log('[llm] Switching to max_completion_tokens');
              ({ res, json } = await attempt({ ...base, max_completion_tokens: 600 }));
            } else if(/max_completion_tokens/i.test(text) && /extra_forbidden/i.test(text)) {
              console.log('[llm] Server forbids max_completion_tokens; retrying with max_tokens only');
              ({ res, json } = await attempt({ ...base, max_tokens: 600 }));
            } else if(/extra_forbidden/i.test(text) && /max_tokens/i.test(text)) {
              console.log('[llm] Server forbids max_tokens; retrying with max_completion_tokens');
              ({ res, json } = await attempt({ ...base, max_completion_tokens: 600 }));
            } else {
              console.warn('[llm] 400 body', text);
            }
        }
        if(!res.ok){ console.warn('[llm] http', res.status); return null; }
        const answer = json?.choices?.[0]?.message?.content?.trim();
        return answer || null;
      } catch(e){ console.warn('[llm] fetch error', e); return null; }
    }

    const speakBtn = document.getElementById('speak');
    const answerBox = document.getElementById('answer');
    const inputEl = document.getElementById('text');
    speakBtn.addEventListener('click', async () => {
      const question = inputEl.value.trim();
      if(!question) return;
      answerBox.style.display = 'none';
      answerBox.textContent='';
      status.textContent = 'Thinking...';
      let finalText = question;
      let usedLLM = false;
      try {
        const llm = await fetchLLMAnswer(question);
        if (llm) { finalText = llm; usedLLM = true; }
        status.textContent = 'Synthesizing...';
        
        // (TTS section reused below)
        const text = finalText;
  const words = [];
  const wtimes = [];
  const wdurations = [];
  // We'll let TalkingHead derive visemes from words; Azure viseme IDs don't match Oculus set.
  synthesizer.visemeReceived = null;
        synthesizer.wordBoundary = (s, e) => {
          const tMs = e.audioOffset / 10000;
          words.push(e.text);
          wtimes.push(tMs);
          wdurations.push(e.duration / 10000);
        };

        const result = await new Promise((resolve, reject) => {
          synthesizer.speakTextAsync(text, r => resolve(r), err => reject(err));
        });
        // If failed, surface details
        if (result.reason === SpeechSDK.ResultReason.Canceled) {
          const cancel = SpeechSDK.CancellationDetails.fromResult(result);
          console.error('[azure] Canceled:', cancel.reason, cancel.errorDetails);
          throw new Error(cancel.errorDetails || 'Azure synthesis canceled');
        }
        if (result.reason !== SpeechSDK.ResultReason.SynthesizingAudioCompleted) {
          console.error('[azure] synthesis failed reason', result.reason);
          throw new Error('Azure synthesis failed');
        }
        if (!result.audioData || result.audioData.byteLength === 0) {
          console.error('[azure] Empty audioData received. Retrying with different format...');
          speechConfig.speechSynthesisOutputFormat = SpeechSDK.SpeechSynthesisOutputFormat.Riff16Khz16BitMonoPcm;
          const retry = await new Promise((resolve, reject) => {
            synthesizer.speakTextAsync(text, r => resolve(r), err => reject(err));
          });
            if (retry.audioData && retry.audioData.byteLength) {
              result.audioData = retry.audioData;
              console.log('[azure] Retry succeeded with 16kHz format');
            } else {
              throw new Error('Azure synthesis produced no audio');
            }
        }
        const audioData = result.audioData.slice(0); // clone
        const audioCtx = head.audioCtx || new (window.AudioContext || window.webkitAudioContext)();
        if (audioCtx.state === 'suspended') {
          try { await audioCtx.resume(); } catch(e){ console.warn('[diag] resume audioCtx failed', e); }
        }
        let buffer;
        let decodeErr;
        try {
          buffer = await audioCtx.decodeAudioData(audioData);
        } catch(e) {
          decodeErr = e;
          console.warn('[azure] decodeAudioData failed (maybe MP3 fallback). Using HTMLAudioElement route.', e);
        }
        if (!buffer && window.USE_MP3_FALLBACK) {
          const blob = new Blob([audioData], {type:'audio/mpeg'});
          const url = URL.createObjectURL(blob);
          const audioEl = document.getElementById('azure-audio-test') || Object.assign(document.createElement('audio'), {id:'azure-audio-test'});
          audioEl.controls = true;
          audioEl.src = url;
          document.body.appendChild(audioEl);
          audioEl.play().then(()=>console.log('[azure] HTMLAudioElement playback started')).catch(err=>console.warn('[azure] HTMLAudioElement play error', err));
          // We still build a silent buffer placeholder so speakAudio path doesn't crash; user can rely on direct playback.
          buffer = audioCtx.createBuffer(1, 1, audioCtx.sampleRate);
        }
        const audioObj = {
          // Pass AudioBuffer directly (NOT inside an array) so TalkingHead uses it as-is.
          audio: buffer,
          words,
          wtimes,
          wdurations
          // No visemes: library will compute from words using english module.
        };
        // Diagnostics: compute RMS to ensure we have non‑zero samples
  const data = buffer.getChannelData(0);
        let sum=0; for(let i=0;i<data.length;i+=Math.floor(data.length/5000)||1){ const v=data[i]; sum+=v*v; }
        const rms = Math.sqrt(sum / (data.length/ (Math.floor(data.length/5000)||1) ));
        console.log('[diag][azure] audio buffer stats', { samples: data.length, rms: +rms.toFixed(5) });
        if (rms < 0.00001) console.warn('[diag][azure] Very low RMS (nearly silence).');

        // Optional direct playback test: set window.PLAY_DIRECT=true before clicking Speak
  const PLAY_DIRECT = (import.meta.env.VITE_PLAY_DIRECT === 'true') || window.PLAY_DIRECT;
  if (PLAY_DIRECT) {
          const src = audioCtx.createBufferSource();
          src.buffer = buffer;
          src.connect(audioCtx.destination);
          src.start();
          console.log('[diag][azure] Direct playback started (bypassing TalkingHead).');
        }

        head.speakAudio(audioObj, {}, null);
        status.textContent = 'Speaking';
        console.log('[diag][azure] queued audio', { wordCount: words.length, durationSec: buffer.duration, bytes: result.audioData.byteLength, usedLLM });
        if (usedLLM) {
          answerBox.textContent = finalText;
          answerBox.style.display = 'block';
        }
        setTimeout(()=>{ console.log('[diag] Pose objects?', {armature: !!head.armature, sceneChildren: head.scene?.children?.length}); }, 2000);
        setTimeout(()=>{ console.log('[diag] After speaking camera pos', head.camera?.position); }, 1000);
      } catch(err) {
        console.error('Synthesis failed:', err);
        status.textContent = 'Error';
      }
    });

    // Helpful: resume audio contexts on first user interaction (iOS/Safari safety)
    const resumeAudio = () => {
      try { head.audioCtx?.state === 'suspended' && head.audioCtx.resume(); } catch(_) {}
  // Only resume TalkingHead audio context; Azure plays directly.
  window.removeEventListener('click', resumeAudio);
    };
    window.addEventListener('click', resumeAudio);

  </script>
</body>
</html>
